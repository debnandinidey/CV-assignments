{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3535fcb5",
   "metadata": {},
   "source": [
    "#### 1. How can each of these parameters be fine-tuned? \n",
    "• Number of hidden layers\n",
    "\n",
    "• Network architecture (network depth)\n",
    "\n",
    "• Each layer&#39;s number of neurons (layer width)\n",
    "\n",
    "• Form of activation\n",
    "\n",
    "• Optimization and learning\n",
    "\n",
    "• Learning rate and decay schedule\n",
    "\n",
    "• Mini batch size\n",
    "\n",
    "• Algorithms for optimization\n",
    "\n",
    "• The number of epochs (and early stopping criteria)\n",
    "\n",
    "• Overfitting that be avoided by using regularization techniques.\n",
    "\n",
    "• L2 normalization\n",
    "\n",
    "• Drop out layers\n",
    "• Data augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a86aeb3",
   "metadata": {},
   "source": [
    "Number of hidden layers: The number of hidden layers can be adjusted to increase or decrease the complexity and capacity of the model. Adding more layers can allow the model to learn more complex representations, but too many layers can lead to overfitting.\n",
    "\n",
    "Network architecture: The overall network architecture, including the arrangement and connectivity of the layers, can be modified to suit the specific problem and data characteristics. Different architectures, such as deep neural networks or convolutional neural networks, can be explored.\n",
    "\n",
    "Number of neurons in each layer: The number of neurons in each layer, also known as the layer width, can be adjusted to control the capacity and expressiveness of the model. Increasing the number of neurons can enhance the model's ability to capture complex patterns, but it also increases the computational complexity.\n",
    "\n",
    "Form of activation: The activation function used in each layer can be chosen based on the specific requirements of the problem. Different activation functions, such as sigmoid, ReLU, or tanh, have different characteristics and can impact the model's ability to learn and converge.\n",
    "\n",
    "Optimization and learning: Various optimization algorithms, such as stochastic gradient descent (SGD) or Adam, can be used to update the model's parameters during training. The learning algorithm's hyperparameters, such as the momentum or weight decay, can be adjusted to control the learning process.\n",
    "\n",
    "Learning rate and decay schedule: The learning rate determines the step size at each parameter update. Fine-tuning the learning rate and its decay schedule can help balance the trade-off between learning speed and convergence.\n",
    "\n",
    "Mini-batch size: The mini-batch size, which determines the number of samples used in each parameter update, can be adjusted. Larger mini-batches can speed up training, but too large of a batch size may result in suboptimal generalization.\n",
    "\n",
    "Algorithms for optimization: Different optimization algorithms like Adam, RMSprop, or SGD with momentum can be used to update the model's parameters.\n",
    "\n",
    "Number of epochs and early stopping criteria: The number of training iterations and early stopping criteria are chosen to avoid underfitting or overfitting.\n",
    "\n",
    "Regularization techniques: L2 regularization (weight decay) helps prevent overfitting by adding a penalty term to the loss function.\n",
    "\n",
    "Dropout layers: Dropout randomly deactivates neurons or connections during training to prevent overfitting and promote robustness.\n",
    "\n",
    "Data augmentation: Applying random transformations to the training data increases its diversity and improves the model's generalization ability.\n",
    "\n",
    "These techniques allow for fine-tuning the model's performance, avoiding overfitting, and improving its ability to generalize.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6bcb30",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
