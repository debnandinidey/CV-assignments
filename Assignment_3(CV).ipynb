{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e616f27f",
   "metadata": {},
   "source": [
    "1. After each stride-2 conv, why do we double the number of filters?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d2ef4b3",
   "metadata": {},
   "source": [
    "Doubling the number of filters after each stride-2 convolution in a convolutional neural network helps increase the network's depth and capacity. This strategy allows the network to capture more complex and abstract features at deeper layers and maintain expressive power. It also provides a larger set of learnable parameters, improving the model's ability to capture patterns and variations in the data. Overall, it enhances the network's representational capacity and enables learning of intricate and hierarchical features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62f58d2",
   "metadata": {},
   "source": [
    "2. Why do we use a larger kernel with MNIST (with simple cnn) in the first conv?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a33cbc2f",
   "metadata": {},
   "source": [
    "Using a larger kernel in the first convolutional layer of a simple CNN for the MNIST dataset allows the network to capture more localized and detailed features from the input images. The MNIST dataset consists of grayscale images of handwritten digits, which have relatively simple and well-defined patterns. By using a larger kernel size, such as 5x5 or 7x7, the network can consider a broader context and capture more intricate details of the digits. This helps improve the model's ability to differentiate between different digits and achieve higher accuracy in classification tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "657b4113",
   "metadata": {},
   "source": [
    "3. What data is saved by ActivationStats for each layer?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79bcba80",
   "metadata": {},
   "source": [
    "The ActivationStats module in deep learning frameworks such as PyTorch or TensorFlow can save various information about the activations (outputs) of each layer during the forward pass of the neural network. The specific data saved by ActivationStats can vary depending on the implementation and configuration, but typically it includes:\n",
    "\n",
    "Activation values: The actual output values of the activations for each layer. These values represent the results of applying the activation function to the inputs.\n",
    "\n",
    "Histograms: Statistics about the distribution of activation values, such as the minimum, maximum, mean, and standard deviation. Histograms provide insights into the spread and range of activation values.\n",
    "\n",
    "Gradients: The gradients of the activations with respect to the loss function. These gradients are important for backpropagation and updating the network's parameters during training.\n",
    "\n",
    "Activation sizes: The shape and size of the activation tensors for each layer. This information helps in understanding the dimensions and structure of the network.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525cfc83",
   "metadata": {},
   "source": [
    "4. How do we get a learner&#39;s callback after they&#39;ve completed training?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe34ce1",
   "metadata": {},
   "source": [
    "In machine learning frameworks such as PyTorch or TensorFlow, a learner's callback function can be executed after the completion of training by utilizing the appropriate callback mechanism provided by the framework.\n",
    "\n",
    "In PyTorch, we can use the torch.nn.Module class's register_forward_hook method to register a callback function that will be called after each forward pass of the network. This allows us to perform custom operations or computations based on the intermediate outputs of the model during training.\n",
    "\n",
    "In TensorFlow, we can use the tf.keras.callbacks module to define custom callbacks that will be executed at different stages of the training process. For example, we can define a custom callback class that inherits from tf.keras.callbacks.Callback and overrides the appropriate methods such as on_train_end() to perform actions after training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7890bd",
   "metadata": {},
   "source": [
    "5. What are the drawbacks of activations above zero?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25af327b",
   "metadata": {},
   "source": [
    "Non-zero activations in a neural network can have drawbacks such as saturation, overfitting, increased computational cost, and increased memory usage. Techniques like regularization, dropout, batch normalization, and using activation functions like ReLU can help mitigate these drawbacks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fbc294b",
   "metadata": {},
   "source": [
    "6.Draw up the benefits and drawbacks of practicing in larger batches?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16938a9",
   "metadata": {},
   "source": [
    "Benefits of practicing in larger batches:\n",
    "\n",
    "Increased computational efficiency: Training with larger batches allows for more efficient use of computational resources, as the operations can be parallelized across multiple examples.\n",
    "Smoother convergence: Larger batches can provide a smoother optimization process, leading to more stable and consistent updates to the model parameters.\n",
    "Better generalization: In some cases, training with larger batches can result in better generalization performance, as it provides a more representative sample of the overall dataset.\n",
    "Drawbacks of practicing in larger batches:\n",
    "\n",
    "Increased memory requirements: Larger batches require more memory to store the intermediate activations and gradients during the training process.\n",
    "Slower updates: Larger batches result in fewer updates to the model parameters per unit of time, which can slow down the training process and make it less responsive to changes in the data.\n",
    "Potential for convergence to suboptimal solutions: Training with larger batches may result in convergence to suboptimal solutions or flat minima, as the larger batch size can smooth out the landscape of the optimization problem.\n",
    "It's important to note that the benefits and drawbacks of practicing in larger batches can vary depending on the specific dataset, model architecture, and optimization algorithm used. The choice of batch size should be carefully considered based on the specific requirements and constraints of the problem at hand."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4546f2d",
   "metadata": {},
   "source": [
    "7. Why should we avoid starting training with a high learning rate?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d60568c",
   "metadata": {},
   "source": [
    "Starting with a high learning rate can lead to unstable optimization and difficulty in convergence. It may cause overshooting the optimal solution and skipping local minima. Using learning rate schedules or decay can help mitigate these issues."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42920645",
   "metadata": {},
   "source": [
    "8. What are the pros of studying with a high rate of learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e374b0",
   "metadata": {},
   "source": [
    "Studying with a high learning rate can lead to faster initial progress and quicker convergence to a satisfactory solution. It allows for larger updates to the model parameters, which can be beneficial in certain scenarios with well-behaved loss landscapes. However, it is important to carefully balance the learning rate to avoid overshooting or instability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7845b566",
   "metadata": {},
   "source": [
    "9. Why do we want to end the training with a low learning rate?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f225e4ac",
   "metadata": {},
   "source": [
    "Ending the training with a low learning rate allows for fine-tuning and better optimization of the model. It helps the model converge to a more precise and accurate solution by making smaller and more controlled updates to the parameters. This can help the model avoid overshooting the optimal solution and stabilize the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5141ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
