{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a45b6b4a",
   "metadata": {},
   "source": [
    "#### 1. What is the difference between TRAINABLE and NON-TRAINABLE PARAMETERS?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07db329a",
   "metadata": {},
   "source": [
    "Trainable parameters, also known as learnable parameters or model parameters, are the variables in a machine learning model that are adjusted during the training process to minimize the loss function and improve the model's performance. These parameters are learned from the training data through optimization algorithms such as gradient descent. Examples of trainable parameters include the weights and biases of the neurons in a neural network.\n",
    "\n",
    "On the other hand, non-trainable parameters, also known as fixed parameters or hyperparameters, are the settings or configurations of the machine learning model that are set by the user before training and remain constant throughout the training process. Non-trainable parameters control the behavior and characteristics of the model but are not updated based on the training data. Examples of non-trainable parameters include the learning rate, batch size, number of layers, activation functions, and regularization parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139d1138",
   "metadata": {},
   "source": [
    "#### 2. In the CNN architecture, where does the DROPOUT LAYER go?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20861805",
   "metadata": {},
   "source": [
    "In a Convolutional Neural Network (CNN) architecture, the Dropout layer is typically inserted after one or more convolutional layers or fully connected layers. The purpose of the Dropout layer is to mitigate overfitting by randomly dropping out (setting to zero) a fraction of the output features or activations of the previous layer during training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16857fd",
   "metadata": {},
   "source": [
    "#### 3. What is the optimal number of hidden layers to stack?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce570d5",
   "metadata": {},
   "source": [
    "If data is less complex and is having fewer dimensions or features then neural networks with 1 to 2 hidden layers would work. If data is having large dimensions or features then to get an optimum solution, 3 to 5 hidden layers can be used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35602c4a",
   "metadata": {},
   "source": [
    "#### 4. In each layer, how many secret units or filters should there be?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "807d18ef",
   "metadata": {},
   "source": [
    "The number of hidden units or filters in each layer of a neural network depends on the complexity of the problem, available data, and computational resources. There is no optimal number, but it is recommended to balance complexity, data availability, and computational limitations. Starting with a smaller number and gradually increase complexity if needed. Experimentation and model evaluation are crucial for determining the appropriate number of units or filters in each layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be32acef",
   "metadata": {},
   "source": [
    "#### 5. What should your initial learning rate be?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f9b962",
   "metadata": {},
   "source": [
    "The initial learning rate in a neural network depends on various factors such as the problem at hand, the size of the dataset, and the network architecture. There is no one-size-fits-all answer for the optimal initial learning rate. However, a common practice is to start with a moderate learning rate, such as 0.1, and then adjust it based on the observed performance and convergence behavior of the model during training. Learning rate schedules, such as learning rate decay or adaptive learning rate methods, can also be used to dynamically adjust the learning rate during training. Experimentation and monitoring the model's performance are crucial to finding an appropriate initial learning rate for a specific task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ff2742",
   "metadata": {},
   "source": [
    "#### 6. What do you do with the activation function?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a2c319",
   "metadata": {},
   "source": [
    "In a neural network, the activation function is applied to the output of each neuron in a layer. It introduces non-linearity to the network, allowing it to learn complex patterns and make nonlinear transformations of the input data. The activation function determines the output value of a neuron based on its weighted sum of inputs.usually activation functions are chosen to be differentiable, allowing for gradient-based optimization algorithms to be applied during training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b313a0",
   "metadata": {},
   "source": [
    "#### 7. What is NORMALIZATION OF DATA?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b085a9",
   "metadata": {},
   "source": [
    "Normalization of data refers to the process of transforming the values of variables to a standard scale, typically between 0 and 1 or with a mean of 0 and a standard deviation of 1. The purpose of normalization is to bring the data into a consistent range, which can help improve the performance and convergence of machine learning algorithms.Normalization is particularly useful when the input variables have different scales or units, as it ensures that no variable dominates the learning process simply because of its larger magnitude. By normalizing the data, variables with larger values do not overshadow variables with smaller values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c04a644",
   "metadata": {},
   "source": [
    "#### 8. What is IMAGE AUGMENTATION and how does it work?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88ea8d1",
   "metadata": {},
   "source": [
    "Image augmentation is a technique used in deep learning to increase the size of a training dataset by applying various transformations to the images. It helps in improving the model's generalization and robustness by introducing variations in the data. Common techniques include flipping, rotation, scaling, translation, brightness/contrast adjustment, and noise addition. By augmenting the data, the model learns from a wider range of variations, reducing overfitting and improving its ability to handle different scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d729b0d",
   "metadata": {},
   "source": [
    "#### 9. What is DECLINE IN LEARNING RATE?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d40168",
   "metadata": {},
   "source": [
    "Learning rate decay, also known as a decline in learning rate, refers to the gradual reduction of the learning rate during the training process of a machine learning model. The learning rate determines the step size at which the model updates its parameters based on the computed gradients.\n",
    "\n",
    "Decaying the learning rate over time can be beneficial for several reasons. Initially, a higher learning rate allows for faster convergence and exploration of the parameter space. However, as training progresses, it is often desirable to reduce the learning rate to fine-tune the model's performance and improve its ability to find the optimal solution. A decaying learning rate can help prevent overshooting or oscillations around the optimal solution, leading to better convergence and more stable training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c38fcd37",
   "metadata": {},
   "source": [
    "#### 10.What does EARLY STOPPING CRITERIA mean?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbcb8f00",
   "metadata": {},
   "source": [
    "Early stopping is a technique used during the training of a machine learning model to prevent overfitting and determine the optimal point to stop training. Early stopping criteria refer to the conditions or metrics used to decide when to stop the training process.\n",
    "\n",
    "The goal of early stopping is to find the point at which the model's performance on a validation dataset starts to deteriorate, indicating that further training may lead to overfitting. Overfitting occurs when a model becomes too specialized to the training data and performs poorly on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b281f8f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
